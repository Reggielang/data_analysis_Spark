1.spark是一种基于（Scala语言开发的）内存的快速，通用，可扩展的大数据分析计算引擎，存储是没有的。

2.spark是侧重于内存计算，处理优化机制。可以独立的存在的。

3.一次性数据计算： 框架再处理数据的时候，会从存储设备中读取数据，进行逻辑操作，
然后将处理的结果重新存储到介质中。并且由于计算简单，所以复杂逻辑的时候，性能不高。

4.spark数据计算放入内存，效率非常高。 可是如果部署在共享的集群当中，可能有种资源不足的问题。
所以不适合和Hadoop堆栈的其他组件一起使用。

5.spark和Hadoop的根本差异是多个作业之间的数据通信问题，
spark多个作业之间的通信是基于内存，Hadoop是基于磁盘。

6.但是spark在实际生产环境中，可能由于内存资源不足，导致job执行失败，此时mapreduce是更好的选择，
所以spark不能完全替代MR。